{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV vs Parquet Benchmark (Notebook Version)\n",
        "\n",
        "This notebook mirrors the logic implemented in `scripts/format_benchmark.py`. It combines data cleaning, feature engineering, metric computation, and persistence utilities with additional logging so that the full workflow can be executed interactively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "The next cell initialises logging and imports all required dependencies. Where optional libraries are unavailable (for example, CodeCarbon or Plotly) the notebook gracefully falls back to safe defaults.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import time\n",
        "import tracemalloc\n",
        "import logging\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(\"format_benchmark_notebook\")\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt  # type: ignore\n",
        "    logger.info(\"Matplotlib loaded successfully.\")\n",
        "except Exception as plot_error:  # pragma: no cover - optional dependency\n",
        "    plt = None  # type: ignore\n",
        "    logger.warning(\"Matplotlib unavailable: %s\", plot_error)\n",
        "\n",
        "try:\n",
        "    import plotly.express as px  # type: ignore\n",
        "    logger.info(\"Plotly loaded successfully.\")\n",
        "except Exception as plotly_error:  # pragma: no cover - optional dependency\n",
        "    px = None  # type: ignore\n",
        "    logger.warning(\"Plotly unavailable: %s\", plotly_error)\n",
        "\n",
        "try:\n",
        "    from codecarbon import EmissionsTracker\n",
        "    logger.info(\"CodeCarbon EmissionsTracker is available.\")\n",
        "except Exception as tracker_error:  # pragma: no cover - optional dependency\n",
        "    EmissionsTracker = None  # type: ignore\n",
        "    logger.warning(\"CodeCarbon unavailable: %s\", tracker_error)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core data structures\n",
        "\n",
        "The benchmark captures detailed metadata about each task in the pipeline. The following cell defines the dataclasses used throughout the workflow along with the constants required for energy estimation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "PREFERRED_POWER_KW = 0.15\n",
        "EMISSIONS_FACTOR_KG_PER_KWH = 0.4\n",
        "\n",
        "@dataclass\n",
        "class TaskBreakdown:\n",
        "    task_id: str\n",
        "    task_label: str\n",
        "    duration_s: float\n",
        "    energy_kwh: float\n",
        "    emissions_kg: float\n",
        "    output_rows: int\n",
        "    memory_peak_mb: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PipelineResult:\n",
        "    format: str\n",
        "    runtime_s: float\n",
        "    energy_kwh: float\n",
        "    emissions_kg: float\n",
        "    row_count: int\n",
        "    output_path: str\n",
        "    output_size_bytes: int\n",
        "    task_breakdown: List[TaskBreakdown]\n",
        "    error: Optional[str] = None\n",
        "\n",
        "    def to_summary_dict(self) -> Dict[str, object]:\n",
        "        payload = asdict(self)\n",
        "        payload[\"task_breakdown\"] = [asdict(task) for task in self.task_breakdown]\n",
        "        return payload\n",
        "\n",
        "    def compact_dict(self) -> Dict[str, object]:\n",
        "        summary = asdict(self)\n",
        "        summary.pop(\"task_breakdown\")\n",
        "        return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning helpers and task registry\n",
        "\n",
        "These helper functions prepare the raw CSV/Parquet sources and define the analysis tasks computed for each merged dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def estimate_energy_and_emissions(duration_s: float) -> Tuple[float, float]:\n",
        "    \"\"\"Return an estimated energy usage (kWh) and emissions (kg CO2).\"\"\"\n",
        "    energy_kwh = (duration_s * PREFERRED_POWER_KW) / 3600.0\n",
        "    emissions_kg = energy_kwh * EMISSIONS_FACTOR_KG_PER_KWH\n",
        "    return energy_kwh, emissions_kg\n",
        "\n",
        "\n",
        "def _series_with_default(df: pd.DataFrame, column: str, default) -> pd.Series:\n",
        "    \"\"\"Return a Series for ``column`` ensuring vector semantics.\"\"\"\n",
        "    if column in df:\n",
        "        series = df[column]\n",
        "        if isinstance(series, pd.Series):\n",
        "            return series\n",
        "        return pd.Series([series] * len(df), index=df.index)\n",
        "    if len(df.index) == 0:\n",
        "        return pd.Series(dtype=\"object\")\n",
        "    return pd.Series([default] * len(df), index=df.index)\n",
        "\n",
        "\n",
        "def clean_books(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean the books dataset in place and return it.\"\"\"\n",
        "    logger.info(\"Cleaning books dataframe with %d rows\", len(df))\n",
        "    authors = _series_with_default(df, \"Authors\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
        "    df[\"Authors\"] = authors.str.title()\n",
        "    df[\"Publisher\"] = _series_with_default(df, \"Publisher\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
        "    df[\"Categories\"] = _series_with_default(df, \"Categories\", \"misc\").fillna(\"misc\").astype(str)\n",
        "    df[\"PublishedDate\"] = pd.to_datetime(df.get(\"PublishedDate\"), errors=\"coerce\")\n",
        "    df[\"RatingsCount\"] = pd.to_numeric(_series_with_default(df, \"RatingsCount\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"AverageRating\"] = pd.to_numeric(df.get(\"AverageRating\"), errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_reviews(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean the reviews dataset in place and return it.\"\"\"\n",
        "    logger.info(\"Cleaning reviews dataframe with %d rows\", len(df))\n",
        "    df.rename(columns={\"profileName\": \"ProfileName\"}, inplace=True)\n",
        "    df[\"review/text\"] = _series_with_default(df, \"review/text\", \"\").fillna(\"\").astype(str)\n",
        "    review_scores = pd.to_numeric(_series_with_default(df, \"review/score\", math.nan), errors=\"coerce\")\n",
        "    mean_score = review_scores.mean() if not review_scores.dropna().empty else 0.0\n",
        "    df[\"review/score\"] = review_scores.fillna(mean_score)\n",
        "    df[\"review/time\"] = pd.to_datetime(df.get(\"review/time\"), unit=\"s\", errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def enrich_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    enriched = df.copy()\n",
        "    enriched[\"review_length\"] = enriched[\"review/text\"].astype(str).str.split().map(len)\n",
        "    enriched[\"Categories\"] = _series_with_default(enriched, \"Categories\", \"misc\").fillna(\"misc\")\n",
        "    enriched[\"CategoriesList\"] = (\n",
        "        enriched[\"Categories\"].astype(str).str.split(\"|\").apply(lambda values: [v.strip().lower() for v in values if v])\n",
        "    )\n",
        "    return enriched\n",
        "\n",
        "\n",
        "TASK_REGISTRY: List[Tuple[str, str, Callable[[pd.DataFrame], pd.DataFrame]]] = [\n",
        "    (\n",
        "        \"avg_rating_per_author\",\n",
        "        \"Average rating per author\",\n",
        "        lambda df: df.groupby(\"Authors\")[\"review/score\"].mean().reset_index().rename(\n",
        "            columns={\"review/score\": \"average_rating\"}\n",
        "        ).sort_values(\"average_rating\", ascending=False),\n",
        "    ),\n",
        "    (\n",
        "        \"reviews_per_publisher\",\n",
        "        \"Reviews per publisher\",\n",
        "        lambda df: df.groupby(\"Publisher\")[\"Id\"].count().reset_index().rename(\n",
        "            columns={\"Id\": \"review_count\"}\n",
        "        ).sort_values(\"review_count\", ascending=False),\n",
        "    ),\n",
        "    (\n",
        "        \"top_categories\",\n",
        "        \"Top 10 most-reviewed categories\",\n",
        "        lambda df: df.explode(\"CategoriesList\").groupby(\"CategoriesList\")[\"Id\"].count().reset_index().rename(\n",
        "            columns={\"CategoriesList\": \"Category\", \"Id\": \"review_count\"}\n",
        "        ).sort_values(\"review_count\", ascending=False).head(10),\n",
        "    ),\n",
        "    (\n",
        "        \"avg_review_length\",\n",
        "        \"Average review length\",\n",
        "        lambda df: pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"metric\": \"average_review_length\",\n",
        "                    \"value\": df[\"review_length\"].mean(),\n",
        "                }\n",
        "            ]\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"top_keywords\",\n",
        "        \"Most frequent review keywords\",\n",
        "        lambda df: pd.DataFrame(\n",
        "            Counter(\" \".join(df[\"review/text\"]).lower().split()).most_common(20),\n",
        "            columns=[\"keyword\", \"occurrences\"],\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metric computation\n",
        "\n",
        "The metric computation cell mirrors the Python module with the addition of logging to report execution time for each analytics task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_metrics(df: pd.DataFrame) -> Tuple[Dict[str, pd.DataFrame], List[TaskBreakdown]]:\n",
        "    metrics: Dict[str, pd.DataFrame] = {}\n",
        "    breakdown: List[TaskBreakdown] = []\n",
        "    for task_id, task_label, task_fn in TASK_REGISTRY:\n",
        "        tracemalloc.start()\n",
        "        task_start = time.perf_counter()\n",
        "        try:\n",
        "            frame = task_fn(df)\n",
        "        finally:\n",
        "            duration = time.perf_counter() - task_start\n",
        "            _, peak_bytes = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.stop()\n",
        "        memory_peak_mb = peak_bytes / (1024 ** 2)\n",
        "        energy_kwh, emissions_kg = estimate_energy_and_emissions(duration)\n",
        "        metrics[task_id] = frame\n",
        "        breakdown.append(\n",
        "            TaskBreakdown(\n",
        "                task_id=task_id,\n",
        "                task_label=task_label,\n",
        "                duration_s=duration,\n",
        "                energy_kwh=energy_kwh,\n",
        "                emissions_kg=emissions_kg,\n",
        "                output_rows=int(len(frame)),\n",
        "                memory_peak_mb=memory_peak_mb,\n",
        "            )\n",
        "        )\n",
        "        logger.info(\"Task '%s' completed in %.3fs\", task_id, duration)\n",
        "    return metrics, breakdown\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline execution utilities\n",
        "\n",
        "The following cell defines utilities for managing the CodeCarbon tracker, persisting metrics, and running individual pipelines. Extensive logging makes it easy to follow the execution flow when the notebook is run interactively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _create_tracker(project_name: str, analysis_dir: Path):\n",
        "    emissions_dir = analysis_dir / \"emissions\"\n",
        "    emissions_dir.mkdir(parents=True, exist_ok=True)\n",
        "    output_file = f\"{project_name}_emissions.jsonl\"\n",
        "\n",
        "    if EmissionsTracker is not None:\n",
        "        try:\n",
        "            return EmissionsTracker(\n",
        "                project_name=project_name,\n",
        "                output_dir=str(emissions_dir),\n",
        "                output_file=output_file,\n",
        "            )\n",
        "        except Exception as tracker_error:  # pragma: no cover - fallback path\n",
        "            logger.warning(\n",
        "                \"Falling back to lightweight tracker because CodeCarbon failed: %s\",\n",
        "                tracker_error,\n",
        "            )\n",
        "\n",
        "    class FallbackTracker:\n",
        "        def __init__(self, project_name: str, target_dir: Path, file_name: str) -> None:\n",
        "            self.project_name = project_name\n",
        "            self.target_dir = target_dir\n",
        "            self.file_name = file_name\n",
        "            self._start: Optional[float] = None\n",
        "\n",
        "        def start(self) -> float:\n",
        "            self._start = time.perf_counter()\n",
        "            return self._start\n",
        "\n",
        "        def stop(self) -> float:\n",
        "            end = time.perf_counter()\n",
        "            duration = end - (self._start or end)\n",
        "            emissions = duration * 0.00012\n",
        "            self._persist(\n",
        "                {\n",
        "                    \"project_name\": self.project_name,\n",
        "                    \"duration_s\": duration,\n",
        "                    \"emissions_kg\": emissions,\n",
        "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
        "                }\n",
        "            )\n",
        "            return emissions\n",
        "\n",
        "        def _persist(self, payload: Dict[str, object]) -> None:\n",
        "            try:\n",
        "                self.target_dir.mkdir(parents=True, exist_ok=True)\n",
        "                with (self.target_dir / self.file_name).open(\"a\", encoding=\"utf-8\") as handle:\n",
        "                    handle.write(json.dumps(payload) + \"\n",
        "\")\n",
        "            except Exception as persist_error:  # pragma: no cover - informational\n",
        "                logger.warning(\"Could not persist fallback emissions data: %s\", persist_error)\n",
        "\n",
        "    return FallbackTracker(project_name, emissions_dir, output_file)\n",
        "\n",
        "\n",
        "def persist_metrics(\n",
        "    format_name: str,\n",
        "    df: pd.DataFrame,\n",
        "    metrics: Dict[str, pd.DataFrame],\n",
        "    output_path: Path,\n",
        "    writer: Callable[[pd.DataFrame, Path], None],\n",
        "    analysis_dir: Path,\n",
        ") -> int:\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    writer(df, output_path)\n",
        "    try:\n",
        "        output_size = output_path.stat().st_size\n",
        "    except FileNotFoundError:\n",
        "        output_size = 0\n",
        "\n",
        "    prefix = f\"{format_name}_{output_path.stem}\"\n",
        "    for name, frame in metrics.items():\n",
        "        target = analysis_dir / f\"{prefix}_{name}.csv\"\n",
        "        frame.to_csv(target, index=False)\n",
        "    logger.info(\"Persisted metrics for format '%s'\", format_name)\n",
        "    return output_size\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    format_name: str,\n",
        "    loader: Callable[[], Tuple[pd.DataFrame, pd.DataFrame]],\n",
        "    writer: Callable[[pd.DataFrame, Path], None],\n",
        "    output_name: str,\n",
        "    project_name: str,\n",
        "    analysis_dir: Path,\n",
        "    outputs_dir: Path,\n",
        ") -> PipelineResult:\n",
        "    tracker = _create_tracker(project_name, analysis_dir)\n",
        "    start = time.perf_counter()\n",
        "    emissions_from_tracker = math.nan\n",
        "    error: Optional[str] = None\n",
        "    merged_df: Optional[pd.DataFrame] = None\n",
        "    output_size_bytes = 0\n",
        "\n",
        "    try:\n",
        "        logger.info(\"[%s] Pipeline starting\", format_name)\n",
        "        tracker.start()\n",
        "        books_df, reviews_df = loader()\n",
        "        books_df = clean_books(books_df)\n",
        "        reviews_df = clean_reviews(reviews_df)\n",
        "        merged_df = enrich_features(\n",
        "            reviews_df.merge(books_df, on=\"Title\", how=\"inner\", suffixes=(\"_review\", \"_book\"))\n",
        "        )\n",
        "        metrics, task_breakdown = compute_metrics(merged_df)\n",
        "        output_size_bytes = persist_metrics(\n",
        "            format_name, merged_df, metrics, outputs_dir / output_name, writer, analysis_dir\n",
        "        )\n",
        "    except Exception as pipeline_error:\n",
        "        error = str(pipeline_error)\n",
        "        task_breakdown = []\n",
        "        logger.exception(\"[%s] Pipeline encountered an issue\", format_name)\n",
        "    finally:\n",
        "        duration = time.perf_counter() - start\n",
        "        try:\n",
        "            emissions_from_tracker = tracker.stop()\n",
        "        except Exception as tracker_error:  # pragma: no cover - fallback path\n",
        "            logger.warning(\"[%s] Unable to obtain emissions from tracker: %s\", format_name, tracker_error)\n",
        "        energy_kwh, estimated_emissions = estimate_energy_and_emissions(duration)\n",
        "        emissions_kg = (\n",
        "            emissions_from_tracker\n",
        "            if isinstance(emissions_from_tracker, (int, float)) and not math.isnan(emissions_from_tracker)\n",
        "            else estimated_emissions\n",
        "        )\n",
        "        if merged_df is None:\n",
        "            output_size_bytes = 0\n",
        "        logger.info(\n",
        "            \"[%s] Pipeline finished in %.3fs (energy %.6fkWh, emissions %.6fkg)\",\n",
        "            format_name,\n",
        "            duration,\n",
        "            energy_kwh,\n",
        "            emissions_kg,\n",
        "        )\n",
        "\n",
        "    return PipelineResult(\n",
        "        format=format_name,\n",
        "        runtime_s=duration,\n",
        "        energy_kwh=energy_kwh,\n",
        "        emissions_kg=emissions_kg,\n",
        "        row_count=int(0 if merged_df is None else len(merged_df)),\n",
        "        output_path=str(outputs_dir / output_name),\n",
        "        output_size_bytes=int(output_size_bytes),\n",
        "        task_breakdown=task_breakdown,\n",
        "        error=error,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I/O helpers\n",
        "\n",
        "These functions take care of loading CSV/Parquet data, writing outputs, and keeping the Parquet copies in sync with the source CSV files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _load_csv(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    return pd.read_csv(data_dir / \"books_data.csv\"), pd.read_csv(data_dir / \"Books_rating.csv\")\n",
        "\n",
        "\n",
        "def _load_parquet(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    return pd.read_parquet(data_dir / \"books_data.parquet\"), pd.read_parquet(data_dir / \"Books_rating.parquet\")\n",
        "\n",
        "\n",
        "def _load_parquet_gzip(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    return pd.read_parquet(data_dir / \"books_data_gzip.parquet\"), pd.read_parquet(\n",
        "        data_dir / \"Books_rating_gzip.parquet\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _write_csv(df: pd.DataFrame, path: Path) -> None:\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "def _write_parquet(df: pd.DataFrame, path: Path) -> None:\n",
        "    try:\n",
        "        df.to_parquet(path, index=False, compression=\"snappy\")\n",
        "    except Exception:  # pragma: no cover - fallback when snappy unavailable\n",
        "        df.to_parquet(path, index=False)\n",
        "\n",
        "\n",
        "def _write_filtered_parquet(df: pd.DataFrame, path: Path) -> None:\n",
        "    important_columns = [\"Id\", \"Title\", \"review/score\", \"review/text\", \"review_length\", \"Authors\", \"Categories\"]\n",
        "    filtered = df[important_columns]\n",
        "    try:\n",
        "        filtered.to_parquet(path, index=False, compression=\"snappy\")\n",
        "    except Exception:  # pragma: no cover\n",
        "        filtered.to_parquet(path, index=False)\n",
        "\n",
        "\n",
        "def _write_parquet_gzip(df: pd.DataFrame, path: Path) -> None:\n",
        "    df.to_parquet(path, index=False, compression=\"gzip\")\n",
        "\n",
        "\n",
        "def _refresh_parquet_copies(data_dir: Path) -> None:\n",
        "    refresh_specs = [\n",
        "        (\"books_data.csv\", \"books_data.parquet\", clean_books),\n",
        "        (\"Books_rating.csv\", \"Books_rating.parquet\", clean_reviews),\n",
        "    ]\n",
        "    for source_name, target_name, cleaner in refresh_specs:\n",
        "        source = data_dir / source_name\n",
        "        target_snappy = data_dir / target_name\n",
        "        target_gzip = data_dir / f\"{Path(target_name).stem}_gzip.parquet\"\n",
        "\n",
        "        needs_refresh = True\n",
        "        if target_snappy.exists() and target_gzip.exists():\n",
        "            try:\n",
        "                source_mtime = source.stat().st_mtime\n",
        "                needs_refresh = (\n",
        "                    source_mtime > target_snappy.stat().st_mtime\n",
        "                    or source_mtime > target_gzip.stat().st_mtime\n",
        "                )\n",
        "            except OSError:\n",
        "                needs_refresh = True\n",
        "            else:\n",
        "                if not needs_refresh:\n",
        "                    continue\n",
        "\n",
        "        logger.info(\"Refreshing Parquet copies for %s\", source_name)\n",
        "        df_full = pd.read_csv(source)\n",
        "        cleaner(df_full)\n",
        "        df_full.to_csv(source, index=False)\n",
        "        try:\n",
        "            df_full.to_parquet(target_snappy, index=False, compression=\"snappy\")\n",
        "        except Exception:\n",
        "            df_full.to_parquet(target_snappy, index=False)\n",
        "        df_full.to_parquet(target_gzip, index=False, compression=\"gzip\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark orchestration\n",
        "\n",
        "The orchestration layer drives each pipeline variant and exports summary files and plots. It mirrors the behaviour of the standalone script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _export_summary(\n",
        "    results: Iterable[PipelineResult],\n",
        "    analysis_dir: Path,\n",
        "    generate_plots: bool,\n",
        ") -> None:\n",
        "    summary_records: List[Dict[str, object]] = []\n",
        "    task_rows: List[Dict[str, object]] = []\n",
        "    for result in results:\n",
        "        summary_records.append(result.compact_dict())\n",
        "        for task in result.task_breakdown:\n",
        "            row = asdict(task)\n",
        "            row[\"format\"] = result.format\n",
        "            task_rows.append(row)\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_records)\n",
        "    summary_path = analysis_dir / \"format_comparison.csv\"\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    logger.info(\"Summary saved to %s\", summary_path)\n",
        "\n",
        "    if generate_plots and plt is not None and not summary_df.empty:\n",
        "        figure_path = analysis_dir / \"format_comparison.png\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        chart_specs = [\n",
        "            (\"runtime_s\", \"Runtime (s)\", \"Runtime by format\", \"#1f77b4\"),\n",
        "            (\"energy_kwh\", \"Energy (kWh)\", \"Energy consumption by format\", \"#ff7f0e\"),\n",
        "            (\"emissions_kg\", \"Emissions (kg CO2)\", \"Carbon emissions by format\", \"#2ca02c\"),\n",
        "        ]\n",
        "        for ax, (column, ylabel, title, color) in zip(axes, chart_specs):\n",
        "            summary_df.plot.bar(x=\"format\", y=column, ax=ax, color=color, legend=False)\n",
        "            ax.set_ylabel(ylabel)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xlabel(\"File format\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(figure_path, dpi=150)\n",
        "        plt.close(fig)\n",
        "        logger.info(\"Matplotlib comparison chart saved to %s\", figure_path)\n",
        "\n",
        "    task_summary_df = pd.DataFrame(task_rows)\n",
        "    task_path = analysis_dir / \"format_task_comparison.csv\"\n",
        "    task_summary_df.to_csv(task_path, index=False)\n",
        "    logger.info(\"Task-level summary saved to %s\", task_path)\n",
        "\n",
        "    if generate_plots and px is not None and not task_summary_df.empty:\n",
        "        plotly_path = analysis_dir / \"task_runtime_comparison.html\"\n",
        "        plotly_fig = px.bar(\n",
        "            task_summary_df,\n",
        "            x=\"task_label\",\n",
        "            y=\"duration_s\",\n",
        "            color=\"format\",\n",
        "            barmode=\"group\",\n",
        "            title=\"Runtime by task and file format\",\n",
        "            labels={\"task_label\": \"Task\", \"duration_s\": \"Runtime (s)\", \"format\": \"Format\"},\n",
        "        )\n",
        "        plotly_fig.write_html(plotly_path)\n",
        "        logger.info(\"Plotly comparison chart saved to %s\", plotly_path)\n",
        "\n",
        "\n",
        "def run_benchmark(data_dir: Path, outputs_dir: Path, analysis_dir: Path, generate_plots: bool = True) -> List[PipelineResult]:\n",
        "    if not (data_dir / \"books_data.csv\").exists() or not (data_dir / \"Books_rating.csv\").exists():\n",
        "        missing = [\n",
        "            path.name\n",
        "            for path in (data_dir / \"books_data.csv\", data_dir / \"Books_rating.csv\")\n",
        "            if not path.exists()\n",
        "        ]\n",
        "        raise FileNotFoundError(\n",
        "            \"Missing required CSV files: \" + \", \".join(missing)\n",
        "        )\n",
        "\n",
        "    outputs_dir.mkdir(parents=True, exist_ok=True)\n",
        "    analysis_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    _refresh_parquet_copies(data_dir)\n",
        "\n",
        "    results: List[PipelineResult] = []\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"csv\",\n",
        "            lambda: _load_csv(data_dir),\n",
        "            _write_csv,\n",
        "            \"merged_books_reviews_csv.csv\",\n",
        "            \"csv_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"parquet\",\n",
        "            lambda: _load_parquet(data_dir),\n",
        "            _write_parquet,\n",
        "            \"merged_books_reviews_parquet.parquet\",\n",
        "            \"parquet_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"parquet_gzip\",\n",
        "            lambda: _load_parquet_gzip(data_dir),\n",
        "            _write_parquet_gzip,\n",
        "            \"merged_books_reviews_parquet_gzip.parquet\",\n",
        "            \"parquet_gzip_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"parquet_filtered\",\n",
        "            lambda: _load_parquet(data_dir),\n",
        "            _write_filtered_parquet,\n",
        "            \"merged_books_reviews_parquet_filtered.parquet\",\n",
        "            \"parquet_filtered_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    _export_summary(results, analysis_dir, generate_plots)\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the benchmark\n",
        "\n",
        "Configure the directories you wish to use, then run the cell below. The try / except / finally structure ensures any failure is logged while still reporting aggregated results collected up to that point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "data_dir = Path(\"data\")\n",
        "outputs_dir = Path(\"outputs\")\n",
        "analysis_dir = Path(\"analysis\")\n",
        "\n",
        "results: List[PipelineResult] = []\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting full benchmark run\")\n",
        "    results = run_benchmark(data_dir=data_dir, outputs_dir=outputs_dir, analysis_dir=analysis_dir)\n",
        "except Exception as exc:\n",
        "    logger.exception(\"Benchmark run failed: %s\", exc)\n",
        "finally:\n",
        "    if results:\n",
        "        logger.info(\"Benchmark produced %d result entries\", len(results))\n",
        "        display(pd.DataFrame([result.compact_dict() for result in results]))\n",
        "    else:\n",
        "        logger.warning(\"No results available to display.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}