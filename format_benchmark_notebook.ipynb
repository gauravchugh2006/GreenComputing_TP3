{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV vs Parquet Benchmark (Notebook Version)\n",
        "\n",
        "This notebook mirrors the logic implemented in `scripts/format_benchmark.py`. It combines data cleaning, feature engineering, metric computation, and persistence utilities with additional logging so that the full workflow can be executed interactively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "The next cell initialises logging and imports all required dependencies. Where optional libraries are unavailable (for example, CodeCarbon or Plotly) the notebook gracefully falls back to safe defaults.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-31 13:40:40,662 [INFO] format_benchmark_notebook - Matplotlib loaded successfully.\n",
            "2025-10-31 13:40:40,824 [INFO] format_benchmark_notebook - Plotly loaded successfully.\n",
            "2025-10-31 13:40:41,337 [INFO] format_benchmark_notebook - CodeCarbon EmissionsTracker is available.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import math\n",
        "import time\n",
        "import tracemalloc\n",
        "import logging\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(\"format_benchmark_notebook\")\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt  # type: ignore\n",
        "    logger.info(\"Matplotlib loaded successfully.\")\n",
        "except Exception as plot_error:  # pragma: no cover - optional dependency\n",
        "    plt = None  # type: ignore\n",
        "    logger.warning(\"Matplotlib unavailable: %s\", plot_error)\n",
        "\n",
        "try:\n",
        "    import plotly.express as px  # type: ignore\n",
        "    logger.info(\"Plotly loaded successfully.\")\n",
        "except Exception as plotly_error:  # pragma: no cover - optional dependency\n",
        "    px = None  # type: ignore\n",
        "    logger.warning(\"Plotly unavailable: %s\", plotly_error)\n",
        "\n",
        "try:\n",
        "    from codecarbon import EmissionsTracker\n",
        "    logger.info(\"CodeCarbon EmissionsTracker is available.\")\n",
        "except Exception as tracker_error:  # pragma: no cover - optional dependency\n",
        "    EmissionsTracker = None  # type: ignore\n",
        "    logger.warning(\"CodeCarbon unavailable: %s\", tracker_error)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core data structures\n",
        "\n",
        "The benchmark captures detailed metadata about each task in the pipeline. The following cell defines the dataclasses used throughout the workflow along with the constants required for energy estimation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "PREFERRED_POWER_KW = 0.15\n",
        "EMISSIONS_FACTOR_KG_PER_KWH = 0.4\n",
        "\n",
        "@dataclass\n",
        "class TaskBreakdown:\n",
        "    task_id: str\n",
        "    task_label: str\n",
        "    duration_s: float\n",
        "    energy_kwh: float\n",
        "    emissions_kg: float\n",
        "    output_rows: int\n",
        "    memory_peak_mb: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PipelineResult:\n",
        "    format: str\n",
        "    runtime_s: float\n",
        "    energy_kwh: float\n",
        "    emissions_kg: float\n",
        "    row_count: int\n",
        "    output_path: str\n",
        "    output_size_bytes: int\n",
        "    task_breakdown: List[TaskBreakdown]\n",
        "    error: Optional[str] = None\n",
        "\n",
        "    def to_summary_dict(self) -> Dict[str, object]:\n",
        "        payload = asdict(self)\n",
        "        payload[\"task_breakdown\"] = [asdict(task) for task in self.task_breakdown]\n",
        "        return payload\n",
        "\n",
        "    def compact_dict(self) -> Dict[str, object]:\n",
        "        summary = asdict(self)\n",
        "        summary.pop(\"task_breakdown\")\n",
        "        return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning helpers and task registry\n",
        "\n",
        "These helper functions prepare the raw CSV/Parquet sources and define the analysis tasks computed for each merged dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_energy_and_emissions(duration_s: float) -> Tuple[float, float]:\n",
        "    \"\"\"Return an estimated energy usage (kWh) and emissions (kg CO2).\"\"\"\n",
        "    energy_kwh = (duration_s * PREFERRED_POWER_KW) / 3600.0\n",
        "    emissions_kg = energy_kwh * EMISSIONS_FACTOR_KG_PER_KWH\n",
        "    return energy_kwh, emissions_kg\n",
        "\n",
        "\n",
        "def _series_with_default(df: pd.DataFrame, column: str, default) -> pd.Series:\n",
        "    \"\"\"Return a Series for ``column`` ensuring vector semantics.\"\"\"\n",
        "    if column in df:\n",
        "        series = df[column]\n",
        "        if isinstance(series, pd.Series):\n",
        "            return series\n",
        "        return pd.Series([series] * len(df), index=df.index)\n",
        "    if len(df.index) == 0:\n",
        "        return pd.Series(dtype=\"object\")\n",
        "    return pd.Series([default] * len(df), index=df.index)\n",
        "\n",
        "\n",
        "def clean_books(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean the books dataset in place and return it.\"\"\"\n",
        "    logger.info(\"Cleaning books dataframe with %d rows\", len(df))\n",
        "    authors = _series_with_default(df, \"Authors\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
        "    df[\"Authors\"] = authors.str.title()\n",
        "    df[\"Publisher\"] = _series_with_default(df, \"Publisher\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
        "    df[\"Categories\"] = _series_with_default(df, \"Categories\", \"misc\").fillna(\"misc\").astype(str)\n",
        "    df[\"PublishedDate\"] = pd.to_datetime(df.get(\"PublishedDate\"), errors=\"coerce\")\n",
        "    df[\"RatingsCount\"] = pd.to_numeric(_series_with_default(df, \"RatingsCount\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"AverageRating\"] = pd.to_numeric(df.get(\"AverageRating\"), errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_reviews(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean the reviews dataset in place and return it.\"\"\"\n",
        "    logger.info(\"Cleaning reviews dataframe with %d rows\", len(df))\n",
        "    df.rename(columns={\"profileName\": \"ProfileName\"}, inplace=True)\n",
        "    df[\"review/text\"] = _series_with_default(df, \"review/text\", \"\").fillna(\"\").astype(str)\n",
        "    review_scores = pd.to_numeric(_series_with_default(df, \"review/score\", math.nan), errors=\"coerce\")\n",
        "    mean_score = review_scores.mean() if not review_scores.dropna().empty else 0.0\n",
        "    df[\"review/score\"] = review_scores.fillna(mean_score)\n",
        "    df[\"review/time\"] = pd.to_datetime(df.get(\"review/time\"), unit=\"s\", errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def enrich_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    enriched = df.copy()\n",
        "    enriched[\"review_length\"] = enriched[\"review/text\"].astype(str).str.split().map(len)\n",
        "    enriched[\"Categories\"] = _series_with_default(enriched, \"Categories\", \"misc\").fillna(\"misc\")\n",
        "    enriched[\"CategoriesList\"] = (\n",
        "        enriched[\"Categories\"].astype(str).str.split(\"|\").apply(lambda values: [v.strip().lower() for v in values if v])\n",
        "    )\n",
        "    return enriched\n",
        "\n",
        "\n",
        "TASK_REGISTRY: List[Tuple[str, str, Callable[[pd.DataFrame], pd.DataFrame]]] = [\n",
        "    (\n",
        "        \"avg_rating_per_author\",\n",
        "        \"Average rating per author\",\n",
        "        lambda df: df.groupby(\"Authors\")[\"review/score\"].mean().reset_index().rename(\n",
        "            columns={\"review/score\": \"average_rating\"}\n",
        "        ).sort_values(\"average_rating\", ascending=False),\n",
        "    ),\n",
        "    (\n",
        "        \"reviews_per_publisher\",\n",
        "        \"Reviews per publisher\",\n",
        "        lambda df: df.groupby(\"Publisher\")[\"Id\"].count().reset_index().rename(\n",
        "            columns={\"Id\": \"review_count\"}\n",
        "        ).sort_values(\"review_count\", ascending=False),\n",
        "    ),\n",
        "    (\n",
        "        \"top_categories\",\n",
        "        \"Top 10 most-reviewed categories\",\n",
        "        lambda df: df.explode(\"CategoriesList\").groupby(\"CategoriesList\")[\"Id\"].count().reset_index().rename(\n",
        "            columns={\"CategoriesList\": \"Category\", \"Id\": \"review_count\"}\n",
        "        ).sort_values(\"review_count\", ascending=False).head(10),\n",
        "    ),\n",
        "    (\n",
        "        \"avg_review_length\",\n",
        "        \"Average review length\",\n",
        "        lambda df: pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"metric\": \"average_review_length\",\n",
        "                    \"value\": df[\"review_length\"].mean(),\n",
        "                }\n",
        "            ]\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"top_keywords\",\n",
        "        \"Most frequent review keywords\",\n",
        "        lambda df: pd.DataFrame(\n",
        "            Counter(\" \".join(df[\"review/text\"]).lower().split()).most_common(20),\n",
        "            columns=[\"keyword\", \"occurrences\"],\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metric computation\n",
        "\n",
        "The metric computation cell mirrors the Python module with the addition of logging to report execution time for each analytics task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(df: pd.DataFrame) -> Tuple[Dict[str, pd.DataFrame], List[TaskBreakdown]]:\n",
        "    metrics: Dict[str, pd.DataFrame] = {}\n",
        "    breakdown: List[TaskBreakdown] = []\n",
        "    for task_id, task_label, task_fn in TASK_REGISTRY:\n",
        "        tracemalloc.start()\n",
        "        task_start = time.perf_counter()\n",
        "        try:\n",
        "            frame = task_fn(df)\n",
        "        finally:\n",
        "            duration = time.perf_counter() - task_start\n",
        "            _, peak_bytes = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.stop()\n",
        "        memory_peak_mb = peak_bytes / (1024 ** 2)\n",
        "        energy_kwh, emissions_kg = estimate_energy_and_emissions(duration)\n",
        "        metrics[task_id] = frame\n",
        "        breakdown.append(\n",
        "            TaskBreakdown(\n",
        "                task_id=task_id,\n",
        "                task_label=task_label,\n",
        "                duration_s=duration,\n",
        "                energy_kwh=energy_kwh,\n",
        "                emissions_kg=emissions_kg,\n",
        "                output_rows=int(len(frame)),\n",
        "                memory_peak_mb=memory_peak_mb,\n",
        "            )\n",
        "        )\n",
        "        logger.info(\"Task '%s' completed in %.3fs\", task_id, duration)\n",
        "    return metrics, breakdown\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline execution utilities\n",
        "\n",
        "The following cell defines utilities for managing the CodeCarbon tracker, persisting metrics, and running individual pipelines. Extensive logging makes it easy to follow the execution flow when the notebook is run interactively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _create_tracker(project_name: str, analysis_dir: Path):\n",
        "    emissions_dir = analysis_dir / \"emissions\"\n",
        "    emissions_dir.mkdir(parents=True, exist_ok=True)\n",
        "    output_file = f\"{project_name}_emissions.jsonl\"\n",
        "\n",
        "    if EmissionsTracker is not None:\n",
        "        try:\n",
        "            return EmissionsTracker(\n",
        "                project_name=project_name,\n",
        "                output_dir=str(emissions_dir),\n",
        "                output_file=output_file,\n",
        "            )\n",
        "        except Exception as tracker_error:  # pragma: no cover - fallback path\n",
        "            logger.warning(\n",
        "                \"Falling back to lightweight tracker because CodeCarbon failed: %s\",\n",
        "                tracker_error,\n",
        "            )\n",
        "\n",
        "    class FallbackTracker:\n",
        "        def __init__(self, project_name: str, target_dir: Path, file_name: str) -> None:\n",
        "            self.project_name = project_name\n",
        "            self.target_dir = target_dir\n",
        "            self.file_name = file_name\n",
        "            self._start: Optional[float] = None\n",
        "\n",
        "        def start(self) -> float:\n",
        "            self._start = time.perf_counter()\n",
        "            return self._start\n",
        "\n",
        "        def stop(self) -> float:\n",
        "            end = time.perf_counter()\n",
        "            duration = end - (self._start or end)\n",
        "            emissions = duration * 0.00012\n",
        "            self._persist(\n",
        "                {\n",
        "                    \"project_name\": self.project_name,\n",
        "                    \"duration_s\": duration,\n",
        "                    \"emissions_kg\": emissions,\n",
        "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
        "                }\n",
        "            )\n",
        "            return emissions\n",
        "\n",
        "        def _persist(self, payload: Dict[str, object]) -> None:\n",
        "            try:\n",
        "                self.target_dir.mkdir(parents=True, exist_ok=True)\n",
        "                with (self.target_dir / self.file_name).open(\"a\", encoding=\"utf-8\") as handle:\n",
        "                    handle.write(json.dumps(payload) + \"\")\n",
        "            except Exception as persist_error:  # pragma: no cover - informational\n",
        "                logger.warning(\"Could not persist fallback emissions data: %s\", persist_error)\n",
        "\n",
        "    return FallbackTracker(project_name, emissions_dir, output_file)\n",
        "\n",
        "\n",
        "def persist_metrics(\n",
        "    format_name: str,\n",
        "    df: pd.DataFrame,\n",
        "    metrics: Dict[str, pd.DataFrame],\n",
        "    output_path: Path,\n",
        "    writer: Callable[[pd.DataFrame, Path], None],\n",
        "    analysis_dir: Path,\n",
        ") -> int:\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    writer(df, output_path)\n",
        "    try:\n",
        "        output_size = output_path.stat().st_size\n",
        "    except FileNotFoundError:\n",
        "        output_size = 0\n",
        "\n",
        "    prefix = f\"{format_name}_{output_path.stem}\"\n",
        "    for name, frame in metrics.items():\n",
        "        target = analysis_dir / f\"{prefix}_{name}.csv\"\n",
        "        frame.to_csv(target, index=False)\n",
        "    logger.info(\"Persisted metrics for format '%s'\", format_name)\n",
        "    return output_size\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    format_name: str,\n",
        "    loader: Callable[[], Tuple[pd.DataFrame, pd.DataFrame]],\n",
        "    writer: Callable[[pd.DataFrame, Path], None],\n",
        "    output_name: str,\n",
        "    project_name: str,\n",
        "    analysis_dir: Path,\n",
        "    outputs_dir: Path,\n",
        ") -> PipelineResult:\n",
        "    tracker = _create_tracker(project_name, analysis_dir)\n",
        "    start = time.perf_counter()\n",
        "    emissions_from_tracker = math.nan\n",
        "    error: Optional[str] = None\n",
        "    merged_df: Optional[pd.DataFrame] = None\n",
        "    output_size_bytes = 0\n",
        "\n",
        "    try:\n",
        "        logger.info(\"[%s] Pipeline starting\", format_name)\n",
        "        tracker.start()\n",
        "        books_df, reviews_df = loader()\n",
        "        books_df = clean_books(books_df)\n",
        "        reviews_df = clean_reviews(reviews_df)\n",
        "        merged_df = enrich_features(\n",
        "            reviews_df.merge(books_df, on=\"Title\", how=\"inner\", suffixes=(\"_review\", \"_book\"))\n",
        "        )\n",
        "        metrics, task_breakdown = compute_metrics(merged_df)\n",
        "        output_size_bytes = persist_metrics(\n",
        "            format_name, merged_df, metrics, outputs_dir / output_name, writer, analysis_dir\n",
        "        )\n",
        "    except Exception as pipeline_error:\n",
        "        error = str(pipeline_error)\n",
        "        task_breakdown = []\n",
        "        logger.exception(\"[%s] Pipeline encountered an issue\", format_name)\n",
        "    finally:\n",
        "        duration = time.perf_counter() - start\n",
        "        try:\n",
        "            emissions_from_tracker = tracker.stop()\n",
        "        except Exception as tracker_error:  # pragma: no cover - fallback path\n",
        "            logger.warning(\"[%s] Unable to obtain emissions from tracker: %s\", format_name, tracker_error)\n",
        "        energy_kwh, estimated_emissions = estimate_energy_and_emissions(duration)\n",
        "        emissions_kg = (\n",
        "            emissions_from_tracker\n",
        "            if isinstance(emissions_from_tracker, (int, float)) and not math.isnan(emissions_from_tracker)\n",
        "            else estimated_emissions\n",
        "        )\n",
        "        if merged_df is None:\n",
        "            output_size_bytes = 0\n",
        "        logger.info(\n",
        "            \"[%s] Pipeline finished in %.3fs (energy %.6fkWh, emissions %.6fkg)\",\n",
        "            format_name,\n",
        "            duration,\n",
        "            energy_kwh,\n",
        "            emissions_kg,\n",
        "        )\n",
        "\n",
        "    return PipelineResult(\n",
        "        format=format_name,\n",
        "        runtime_s=duration,\n",
        "        energy_kwh=energy_kwh,\n",
        "        emissions_kg=emissions_kg,\n",
        "        row_count=int(0 if merged_df is None else len(merged_df)),\n",
        "        output_path=str(outputs_dir / output_name),\n",
        "        output_size_bytes=int(output_size_bytes),\n",
        "        task_breakdown=task_breakdown,\n",
        "        error=error,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I/O helpers\n",
        "\n",
        "These functions take care of loading CSV/Parquet data, writing outputs, and keeping the Parquet copies in sync with the source CSV files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_csv(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    return pd.read_csv(data_dir / \"books_data.csv\"), pd.read_csv(data_dir / \"Books_rating.csv\")\n",
        "\n",
        "\n",
        "def _load_parquet(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    return pd.read_parquet(data_dir / \"books_data.parquet\"), pd.read_parquet(data_dir / \"Books_rating.parquet\")\n",
        "\n",
        "\n",
        "def _load_parquet_gzip(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    return pd.read_parquet(data_dir / \"books_data_gzip.parquet\"), pd.read_parquet(\n",
        "        data_dir / \"Books_rating_gzip.parquet\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _write_csv(df: pd.DataFrame, path: Path) -> None:\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "def _write_parquet(df: pd.DataFrame, path: Path) -> None:\n",
        "    try:\n",
        "        df.to_parquet(path, index=False, compression=\"snappy\")\n",
        "    except Exception:  # pragma: no cover - fallback when snappy unavailable\n",
        "        df.to_parquet(path, index=False)\n",
        "\n",
        "\n",
        "def _write_filtered_parquet(df: pd.DataFrame, path: Path) -> None:\n",
        "    important_columns = [\"Id\", \"Title\", \"review/score\", \"review/text\", \"review_length\", \"Authors\", \"Categories\"]\n",
        "    filtered = df[important_columns]\n",
        "    try:\n",
        "        filtered.to_parquet(path, index=False, compression=\"snappy\")\n",
        "    except Exception:  # pragma: no cover\n",
        "        filtered.to_parquet(path, index=False)\n",
        "\n",
        "\n",
        "def _write_parquet_gzip(df: pd.DataFrame, path: Path) -> None:\n",
        "    df.to_parquet(path, index=False, compression=\"gzip\")\n",
        "\n",
        "\n",
        "def _refresh_parquet_copies(data_dir: Path) -> None:\n",
        "    refresh_specs = [\n",
        "        (\"books_data.csv\", \"books_data.parquet\", clean_books),\n",
        "        (\"Books_rating.csv\", \"Books_rating.parquet\", clean_reviews),\n",
        "    ]\n",
        "    for source_name, target_name, cleaner in refresh_specs:\n",
        "        source = data_dir / source_name\n",
        "        target_snappy = data_dir / target_name\n",
        "        target_gzip = data_dir / f\"{Path(target_name).stem}_gzip.parquet\"\n",
        "\n",
        "        needs_refresh = True\n",
        "        if target_snappy.exists() and target_gzip.exists():\n",
        "            try:\n",
        "                source_mtime = source.stat().st_mtime\n",
        "                needs_refresh = (\n",
        "                    source_mtime > target_snappy.stat().st_mtime\n",
        "                    or source_mtime > target_gzip.stat().st_mtime\n",
        "                )\n",
        "            except OSError:\n",
        "                needs_refresh = True\n",
        "            else:\n",
        "                if not needs_refresh:\n",
        "                    continue\n",
        "\n",
        "        logger.info(\"Refreshing Parquet copies for %s\", source_name)\n",
        "        df_full = pd.read_csv(source)\n",
        "        cleaner(df_full)\n",
        "        df_full.to_csv(source, index=False)\n",
        "        try:\n",
        "            df_full.to_parquet(target_snappy, index=False, compression=\"snappy\")\n",
        "        except Exception:\n",
        "            df_full.to_parquet(target_snappy, index=False)\n",
        "        if target_gzip.exists():\n",
        "            logger.info(\"Skipping gzip Parquet conversion for %s; file already exists\", target_gzip.name)\n",
        "        else:\n",
        "            df_full.to_parquet(target_gzip, index=False, compression=\"gzip\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark orchestration\n",
        "\n",
        "The orchestration layer drives each pipeline variant and exports summary files and plots. It mirrors the behaviour of the standalone script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _export_summary(\n",
        "    results: Iterable[PipelineResult],\n",
        "    analysis_dir: Path,\n",
        "    generate_plots: bool,\n",
        ") -> None:\n",
        "    summary_records: List[Dict[str, object]] = []\n",
        "    task_rows: List[Dict[str, object]] = []\n",
        "    for result in results:\n",
        "        summary_records.append(result.compact_dict())\n",
        "        for task in result.task_breakdown:\n",
        "            row = asdict(task)\n",
        "            row[\"format\"] = result.format\n",
        "            task_rows.append(row)\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_records)\n",
        "    summary_path = analysis_dir / \"format_comparison.csv\"\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    logger.info(\"Summary saved to %s\", summary_path)\n",
        "\n",
        "    if generate_plots and plt is not None and not summary_df.empty:\n",
        "        figure_path = analysis_dir / \"format_comparison.png\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        chart_specs = [\n",
        "            (\"runtime_s\", \"Runtime (s)\", \"Runtime by format\", \"#1f77b4\"),\n",
        "            (\"energy_kwh\", \"Energy (kWh)\", \"Energy consumption by format\", \"#ff7f0e\"),\n",
        "            (\"emissions_kg\", \"Emissions (kg CO2)\", \"Carbon emissions by format\", \"#2ca02c\"),\n",
        "        ]\n",
        "        for ax, (column, ylabel, title, color) in zip(axes, chart_specs):\n",
        "            summary_df.plot.bar(x=\"format\", y=column, ax=ax, color=color, legend=False)\n",
        "            ax.set_ylabel(ylabel)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xlabel(\"File format\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(figure_path, dpi=150)\n",
        "        plt.close(fig)\n",
        "        logger.info(\"Matplotlib comparison chart saved to %s\", figure_path)\n",
        "\n",
        "    task_summary_df = pd.DataFrame(task_rows)\n",
        "    task_path = analysis_dir / \"format_task_comparison.csv\"\n",
        "    task_summary_df.to_csv(task_path, index=False)\n",
        "    logger.info(\"Task-level summary saved to %s\", task_path)\n",
        "\n",
        "    if generate_plots and px is not None and not task_summary_df.empty:\n",
        "        plotly_path = analysis_dir / \"task_runtime_comparison.html\"\n",
        "        plotly_fig = px.bar(\n",
        "            task_summary_df,\n",
        "            x=\"task_label\",\n",
        "            y=\"duration_s\",\n",
        "            color=\"format\",\n",
        "            barmode=\"group\",\n",
        "            title=\"Runtime by task and file format\",\n",
        "            labels={\"task_label\": \"Task\", \"duration_s\": \"Runtime (s)\", \"format\": \"Format\"},\n",
        "        )\n",
        "        plotly_fig.write_html(plotly_path)\n",
        "        logger.info(\"Plotly comparison chart saved to %s\", plotly_path)\n",
        "\n",
        "\n",
        "def run_benchmark(data_dir: Path, outputs_dir: Path, analysis_dir: Path, generate_plots: bool = True) -> List[PipelineResult]:\n",
        "    if not (data_dir / \"books_data.csv\").exists() or not (data_dir / \"Books_rating.csv\").exists():\n",
        "        missing = [\n",
        "            path.name\n",
        "            for path in (data_dir / \"books_data.csv\", data_dir / \"Books_rating.csv\")\n",
        "            if not path.exists()\n",
        "        ]\n",
        "        raise FileNotFoundError(\n",
        "            \"Missing required CSV files: \" + \", \".join(missing)\n",
        "        )\n",
        "\n",
        "    outputs_dir.mkdir(parents=True, exist_ok=True)\n",
        "    analysis_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    _refresh_parquet_copies(data_dir)\n",
        "\n",
        "    results: List[PipelineResult] = []\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"csv\",\n",
        "            lambda: _load_csv(data_dir),\n",
        "            _write_csv,\n",
        "            \"merged_books_reviews_csv.csv\",\n",
        "            \"csv_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"parquet\",\n",
        "            lambda: _load_parquet(data_dir),\n",
        "            _write_parquet,\n",
        "            \"merged_books_reviews_parquet.parquet\",\n",
        "            \"parquet_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"parquet_gzip\",\n",
        "            lambda: _load_parquet_gzip(data_dir),\n",
        "            _write_parquet_gzip,\n",
        "            \"merged_books_reviews_parquet_gzip.parquet\",\n",
        "            \"parquet_gzip_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "    results.append(\n",
        "        run_pipeline(\n",
        "            \"parquet_filtered\",\n",
        "            lambda: _load_parquet(data_dir),\n",
        "            _write_filtered_parquet,\n",
        "            \"merged_books_reviews_parquet_filtered.parquet\",\n",
        "            \"parquet_filtered_pipeline\",\n",
        "            analysis_dir,\n",
        "            outputs_dir,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    _export_summary(results, analysis_dir, generate_plots)\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the benchmark\n",
        "\n",
        "Configure the directories you wish to use, then run the cell below. The try / except / finally structure ensures any failure is logged while still reporting aggregated results collected up to that point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fc024b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-31 13:40:41,475 [INFO] format_benchmark_notebook - Starting full benchmark run path: data\n",
            "2025-10-31 13:40:41,479 [INFO] format_benchmark_notebook - Refreshing Parquet copies for books_data.csv\n",
            "2025-10-31 13:40:44,530 [INFO] format_benchmark_notebook - Cleaning books dataframe with 212404 rows\n",
            "2025-10-31 13:41:06,086 [INFO] format_benchmark_notebook - Refreshing Parquet copies for Books_rating.csv\n",
            "2025-10-31 13:41:44,677 [INFO] format_benchmark_notebook - Cleaning reviews dataframe with 3000000 rows\n",
            "[codecarbon WARNING @ 13:48:24] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 13:48:24] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 13:48:24] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 13:48:26] We saw that you have a Intel(R) Core(TM) Ultra 9 185H but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:48:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 13:48:26] CPU Model on constant consumption mode: Intel(R) Core(TM) Ultra 9 185H\n",
            "[codecarbon WARNING @ 13:48:26] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 13:48:26] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 13:48:26] No GPU found.\n",
            "[codecarbon INFO @ 13:48:26] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: Unspecified\n",
            "            \n",
            "[codecarbon INFO @ 13:48:26] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 13:48:26]   Platform system: Windows-11-10.0.26200-SP0\n",
            "[codecarbon INFO @ 13:48:26]   Python version: 3.12.6\n",
            "[codecarbon INFO @ 13:48:26]   CodeCarbon version: 3.0.8\n",
            "[codecarbon INFO @ 13:48:26]   Available RAM : 31.435 GB\n",
            "[codecarbon INFO @ 13:48:27]   CPU count: 22 thread(s) in 22 physical CPU(s)\n",
            "[codecarbon INFO @ 13:48:27]   CPU model: Intel(R) Core(TM) Ultra 9 185H\n",
            "[codecarbon INFO @ 13:48:27]   GPU count: None\n",
            "[codecarbon INFO @ 13:48:27]   GPU model: None\n",
            "[codecarbon INFO @ 13:48:27] Emissions data (if any) will be saved to file c:\\Users\\Gaurav Chugh\\source\\GreenComputing_TP3\\analysis\\emissions\\csv_pipeline_emissions.jsonl\n",
            "2025-10-31 13:48:27,774 [INFO] format_benchmark_notebook - [csv] Pipeline starting\n",
            "[codecarbon INFO @ 13:48:42] Energy consumed for RAM : 0.000083 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:48:42] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:48:42] Energy consumed for All CPU : 0.000177 kWh\n",
            "[codecarbon INFO @ 13:48:42] 0.000261 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:48:57] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:48:57] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:48:57] Energy consumed for All CPU : 0.000354 kWh\n",
            "[codecarbon INFO @ 13:48:57] 0.000521 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "2025-10-31 13:49:08,845 [INFO] format_benchmark_notebook - Cleaning books dataframe with 212404 rows\n",
            "2025-10-31 13:49:09,035 [INFO] format_benchmark_notebook - Cleaning reviews dataframe with 3000000 rows\n",
            "[codecarbon INFO @ 13:49:13] Energy consumed for RAM : 0.000255 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:49:13] Delta energy consumed for CPU with constant : 0.000187 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:49:13] Energy consumed for All CPU : 0.000541 kWh\n",
            "[codecarbon INFO @ 13:49:13] 0.000796 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:49:28] Energy consumed for RAM : 0.000338 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:49:28] Delta energy consumed for CPU with constant : 0.000180 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:49:28] Energy consumed for All CPU : 0.000721 kWh\n",
            "[codecarbon INFO @ 13:49:29] 0.001059 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:49:43] Energy consumed for RAM : 0.000419 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:49:43] Delta energy consumed for CPU with constant : 0.000174 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:49:44] Energy consumed for All CPU : 0.000896 kWh\n",
            "[codecarbon INFO @ 13:49:44] 0.001315 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:50:01] Energy consumed for RAM : 0.000515 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:50:01] Delta energy consumed for CPU with constant : 0.000206 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:50:01] Energy consumed for All CPU : 0.001102 kWh\n",
            "[codecarbon INFO @ 13:50:01] 0.001617 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:50:18] Energy consumed for RAM : 0.000607 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:50:18] Delta energy consumed for CPU with constant : 0.000195 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:50:18] Energy consumed for All CPU : 0.001296 kWh\n",
            "[codecarbon INFO @ 13:50:18] 0.001903 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:50:33] Energy consumed for RAM : 0.000689 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:50:33] Delta energy consumed for CPU with constant : 0.000176 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:50:33] Energy consumed for All CPU : 0.001473 kWh\n",
            "[codecarbon INFO @ 13:50:33] 0.002162 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:50:33] 0.000961 g.CO2eq/s mean an estimation of 30.30748119664991 kg.CO2eq/year\n",
            "[codecarbon INFO @ 13:50:48] Energy consumed for RAM : 0.000771 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:50:48] Delta energy consumed for CPU with constant : 0.000175 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:50:48] Energy consumed for All CPU : 0.001648 kWh\n",
            "[codecarbon INFO @ 13:50:48] 0.002418 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:51:04] Energy consumed for RAM : 0.000856 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:51:04] Delta energy consumed for CPU with constant : 0.000183 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:51:04] Energy consumed for All CPU : 0.001831 kWh\n",
            "[codecarbon INFO @ 13:51:04] 0.002686 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:51:19] Energy consumed for RAM : 0.000935 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:51:19] Delta energy consumed for CPU with constant : 0.000169 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:51:19] Energy consumed for All CPU : 0.002000 kWh\n",
            "[codecarbon INFO @ 13:51:19] 0.002935 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon WARNING @ 13:55:28] Background scheduler didn't run for a long period (249s), results might be inaccurate\n",
            "[codecarbon INFO @ 13:55:28] Energy consumed for RAM : 0.002319 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:55:28] Delta energy consumed for CPU with constant : 0.002942 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:55:28] Energy consumed for All CPU : 0.004942 kWh\n",
            "[codecarbon INFO @ 13:55:28] 0.007261 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:55:43] Energy consumed for RAM : 0.002401 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:55:43] Delta energy consumed for CPU with constant : 0.000175 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:55:43] Energy consumed for All CPU : 0.005117 kWh\n",
            "[codecarbon INFO @ 13:55:43] 0.007518 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon WARNING @ 13:56:40] Background scheduler didn't run for a long period (56s), results might be inaccurate\n",
            "[codecarbon INFO @ 13:56:40] Energy consumed for RAM : 0.002715 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:56:40] Delta energy consumed for CPU with constant : 0.000667 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:56:40] Energy consumed for All CPU : 0.005784 kWh\n",
            "[codecarbon INFO @ 13:56:40] 0.008499 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon INFO @ 13:56:55] Energy consumed for RAM : 0.002798 kWh. RAM Power : 20.0 W\n",
            "[codecarbon INFO @ 13:56:55] Delta energy consumed for CPU with constant : 0.000178 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 13:56:55] Energy consumed for All CPU : 0.005962 kWh\n",
            "[codecarbon INFO @ 13:56:55] 0.008760 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "2025-10-31 13:57:01,506 [INFO] format_benchmark_notebook - Task 'avg_rating_per_author' completed in 0.343s\n",
            "2025-10-31 13:57:01,870 [INFO] format_benchmark_notebook - Task 'reviews_per_publisher' completed in 0.361s\n",
            "2025-10-31 13:57:09,078 [INFO] format_benchmark_notebook - Task 'top_categories' completed in 7.202s\n",
            "2025-10-31 13:57:09,098 [INFO] format_benchmark_notebook - Task 'avg_review_length' completed in 0.018s\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path(\"data\")\n",
        "outputs_dir = Path(\"outputs\")\n",
        "analysis_dir = Path(\"analysis\")\n",
        "\n",
        "results: List[PipelineResult] = []\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting full benchmark run path: %s\", data_dir)\n",
        "    results = run_benchmark(data_dir=data_dir, outputs_dir=outputs_dir, analysis_dir=analysis_dir)\n",
        "except Exception as exc:\n",
        "    logger.exception(\"Benchmark run failed: %s\", exc)\n",
        "finally:\n",
        "    if results:\n",
        "        logger.info(\"Benchmark produced %d result entries\", len(results))\n",
        "        display(pd.DataFrame([result.compact_dict() for result in results]))\n",
        "    else:\n",
        "        logger.warning(\"No results available to display.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
