{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objectives\n",
        "\n",
        "- See how **storage formats** (CSV vs Parquet) affect performance and energy.\n",
        "- Instrument data pipelines with **CodeCarbon** to measure runtime and CO\u2082.\n",
        "- Compare two runs of the same pipeline that differ only by file format.\n",
        "- Explain results in terms of *I/O*, compression, and greener ETL choices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Context\n",
        "\n",
        "We benchmark the existing CSV-based books and reviews pipeline against a functionally equivalent Parquet pipeline.\n",
        "The goal is to show whether switching to a columnar, compressed format reduces runtime, file size, and estimated emissions for the same analytical workload.\n",
        "Results support a recommendation on greener storage choices for downstream analytics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets Overview\n",
        "\n",
        "- `books.csv` \u2014 bibliographic metadata with fields such as `Title`, `Authors`, `Publisher`, `PublishedDate`, and `Categories`.\n",
        "- `reviews.csv` \u2014 user feedback that includes `Id`, `Title`, `Price`, `User_id`, `profileName`, `review/score`, `review/text`, and timestamps.\n",
        "\n",
        "The helper cell below ensures sample files exist (for a self-contained demo) and previews the first rows of each dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import statistics\n",
        "import textwrap\n",
        "import time\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "DEPENDENCIES = {\"pandas\": \"pandas\", \"matplotlib\": \"matplotlib\", \"pyarrow\": \"pyarrow\", \"numpy\": \"numpy\", \"codecarbon\": \"codecarbon\"}\n",
        "loaded_modules: Dict[str, object] = {}\n",
        "missing_dependencies: List[str] = []\n",
        "\n",
        "for module_name, package_name in DEPENDENCIES.items():\n",
        "    try:\n",
        "        loaded_modules[module_name] = importlib.import_module(module_name)\n",
        "    except ImportError:\n",
        "        missing_dependencies.append(package_name)\n",
        "\n",
        "if missing_dependencies:\n",
        "    print(\"\u26a0\ufe0f Optional dependencies missing:\", \", \".join(sorted(set(missing_dependencies))))\n",
        "else:\n",
        "    print(\"\u2705 All optional dependencies imported successfully.\")\n",
        "\n",
        "pd = loaded_modules.get(\"pandas\")\n",
        "plt = loaded_modules.get(\"matplotlib\").pyplot if loaded_modules.get(\"matplotlib\") else None\n",
        "np = loaded_modules.get(\"numpy\")\n",
        "codecarbon_module = loaded_modules.get(\"codecarbon\")\n",
        "\n",
        "BASE_PATH = Path(\".\")\n",
        "DATA_DIR = BASE_PATH / \"data\"\n",
        "OUTPUTS_DIR = BASE_PATH / \"outputs\"\n",
        "ANALYSIS_DIR = BASE_PATH / \"analysis\"\n",
        "for directory in (DATA_DIR, OUTPUTS_DIR, ANALYSIS_DIR):\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEPENDENCIES_READY = pd is not None and plt is not None\n",
        "if not DEPENDENCIES_READY:\n",
        "    print(\"\u27a1\ufe0f Install the missing packages and re-run the notebook for full functionality.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure demo datasets exist so the pipeline can run end-to-end in any environment.\n",
        "import csv\n",
        "\n",
        "books_path = DATA_DIR / \"books.csv\"\n",
        "reviews_path = DATA_DIR / \"reviews.csv\"\n",
        "\n",
        "def _generate_books() -> List[Dict[str, object]]:\n",
        "    rng = random.Random(42)\n",
        "    titles = [\n",
        "        \"The Pragmatic Programmer\",\n",
        "        \"Clean Code\",\n",
        "        \"Effective Python\",\n",
        "        \"Designing Data-Intensive Applications\",\n",
        "        \"Deep Learning with Python\",\n",
        "        \"Hands-On Machine Learning\",\n",
        "        \"Introduction to Algorithms\",\n",
        "        \"Python Data Science Handbook\",\n",
        "        \"Data Pipelines Pocket Reference\",\n",
        "        \"Building Microservices\",\n",
        "    ]\n",
        "    categories = [\"programming\", \"software engineering\", \"data\", \"machine learning\", \"architecture\"]\n",
        "    publishers = [\"Addison-Wesley\", \"O'Reilly Media\", \"No Starch Press\", \"Manning\"]\n",
        "    authors = [\n",
        "        \"Andrew Hunt\", \"Robert C. Martin\", \"Brett Slatkin\", \"Martin Kleppmann\", \"Francois Chollet\",\n",
        "        \"Aurelien Geron\", \"Thomas H. Cormen\", \"Jake VanderPlas\", \"James Densmore\", \"Sam Newman\",\n",
        "    ]\n",
        "    rows: List[Dict[str, object]] = []\n",
        "    for idx, title in enumerate(titles):\n",
        "        rows.append({\n",
        "            \"Title\": title,\n",
        "            \"Description\": f\"Insightful discussion about {title}.\",\n",
        "            \"Authors\": authors[idx % len(authors)],\n",
        "            \"Publisher\": publishers[idx % len(publishers)],\n",
        "            \"PublishedDate\": datetime(2010 + idx % 10, 1 + (idx % 12), 1 + (idx % 28)).date().isoformat(),\n",
        "            \"Categories\": categories[idx % len(categories)],\n",
        "            \"RatingsCount\": rng.randint(50, 5000),\n",
        "            \"AverageRating\": round(rng.uniform(3.0, 5.0), 2),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def _generate_reviews() -> List[Dict[str, object]]:\n",
        "    rng = random.Random(123)\n",
        "    rows: List[Dict[str, object]] = []\n",
        "    titles = [row[\"Title\"] for row in _generate_books()]\n",
        "    for review_id in range(1, 1001):\n",
        "        title = rng.choice(titles)\n",
        "        rows.append({\n",
        "            \"Id\": review_id,\n",
        "            \"Title\": title,\n",
        "            \"Price\": round(rng.uniform(10, 80), 2),\n",
        "            \"User_id\": rng.randint(1, 500),\n",
        "            \"profileName\": f\"User {rng.randint(1, 500)}\",\n",
        "            \"review/score\": rng.randint(1, 5),\n",
        "            \"review/text\": \" \".join(\n",
        "                rng.choices(\n",
        "                    [\"great\", \"insightful\", \"comprehensive\", \"dense\", \"practical\", \"clear\", \"challenging\"],\n",
        "                    k=rng.randint(8, 30),\n",
        "                )\n",
        "            ),\n",
        "            \"review/time\": int(datetime(2020, rng.randint(1, 12), rng.randint(1, 28)).timestamp()),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "if not books_path.exists() or not reviews_path.exists():\n",
        "    print(\"Creating synthetic CSV assets to keep the notebook self-contained.\")\n",
        "    books_rows = _generate_books()\n",
        "    reviews_rows = _generate_reviews()\n",
        "    if pd is not None:\n",
        "        pd.DataFrame(books_rows).to_csv(books_path, index=False)\n",
        "        pd.DataFrame(reviews_rows).to_csv(reviews_path, index=False)\n",
        "    else:\n",
        "        with books_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as handle:\n",
        "            writer = csv.DictWriter(handle, fieldnames=list(books_rows[0].keys()))\n",
        "            writer.writeheader()\n",
        "            writer.writerows(books_rows)\n",
        "        with reviews_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as handle:\n",
        "            writer = csv.DictWriter(handle, fieldnames=list(reviews_rows[0].keys()))\n",
        "            writer.writeheader()\n",
        "            writer.writerows(reviews_rows)\n",
        "else:\n",
        "    print(\"Reusing existing CSV files from the data/ directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the first rows from each dataset so readers know the schema before processing.\n",
        "if pd is None:\n",
        "    print(\"Pandas is required to preview DataFrames. Install pandas and re-run this cell.\")\n",
        "else:\n",
        "    try:\n",
        "        books_df = pd.read_csv(books_path)\n",
        "        reviews_df = pd.read_csv(reviews_path)\n",
        "    except Exception as load_error:\n",
        "        books_df = None\n",
        "        reviews_df = None\n",
        "        print(f\"Failed to load CSV files: {load_error}\")\n",
        "    else:\n",
        "        display(books_df.head())\n",
        "        display(reviews_df.head())\n",
        "    finally:\n",
        "        if \"books_df\" in locals() and isinstance(books_df, type(pd.DataFrame())):\n",
        "            print(f\"Loaded {len(books_df)} book rows.\")\n",
        "        if \"reviews_df\" in locals() and isinstance(reviews_df, type(pd.DataFrame())):\n",
        "            print(f\"Loaded {len(reviews_df)} review rows.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimental Design\n",
        "\n",
        "1. Load raw CSV assets for books and reviews.\n",
        "2. Clean textual fields and normalise categories/authors.\n",
        "3. Join the datasets on `Title`.\n",
        "4. Compute metrics (ratings per author, reviews per publisher, top categories, review length stats, frequent keywords).\n",
        "5. Persist the merged dataset in the chosen format.\n",
        "\n",
        "### Pipeline A \u2013 CSV\n",
        "### Pipeline B \u2013 Parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "            PIPELINE_RESULTS: List[Dict[str, object]] = []\n",
        "\n",
        "            if not DEPENDENCIES_READY:\n",
        "                print(\"Install pandas and matplotlib to enable the reusable pipeline helpers.\")\n",
        "            else:\n",
        "                def create_tracker(project_name: str):\n",
        "                    emissions_dir = ANALYSIS_DIR / \"emissions\"\n",
        "                    emissions_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    output_file = f\"{project_name}_emissions.jsonl\"\n",
        "                    if codecarbon_module is not None:\n",
        "                        try:\n",
        "                            tracker = codecarbon_module.EmissionsTracker(\n",
        "                                project_name=project_name,\n",
        "                                output_dir=str(emissions_dir),\n",
        "                                output_file=output_file,\n",
        "                            )\n",
        "                            return tracker\n",
        "                        except Exception as tracker_error:\n",
        "                            print(f\"Falling back to lightweight tracker because CodeCarbon initialisation failed: {tracker_error}\")\n",
        "\n",
        "                    class FallbackTracker:\n",
        "                        def __init__(self, project_name: str, target_dir: Path, file_name: str) -> None:\n",
        "                            self.project_name = project_name\n",
        "                            self.target_dir = target_dir\n",
        "                            self.file_name = file_name\n",
        "                            self._start: Optional[float] = None\n",
        "\n",
        "                        def start(self) -> float:\n",
        "                            self._start = time.perf_counter()\n",
        "                            return self._start\n",
        "\n",
        "                        def stop(self) -> float:\n",
        "                            end = time.perf_counter()\n",
        "                            duration = end - (self._start or end)\n",
        "                            emissions = duration * 0.00012\n",
        "                            self._persist(\n",
        "                                {\n",
        "                                    \"project_name\": self.project_name,\n",
        "                                    \"duration_s\": duration,\n",
        "                                    \"emissions_kg\": emissions,\n",
        "                                    \"timestamp\": datetime.utcnow().isoformat(),\n",
        "                                }\n",
        "                            )\n",
        "                            return emissions\n",
        "\n",
        "                        def _persist(self, payload: Dict[str, object]) -> None:\n",
        "                            try:\n",
        "                                self.target_dir.mkdir(parents=True, exist_ok=True)\n",
        "                                with (self.target_dir / self.file_name).open(\"a\", encoding=\"utf-8\") as handle:\n",
        "                                    handle.write(json.dumps(payload) + \"\n",
        "\")\n",
        "                            except Exception as persist_error:\n",
        "                                print(f\"Could not persist fallback emissions data: {persist_error}\")\n",
        "\n",
        "                    return FallbackTracker(project_name, emissions_dir, output_file)\n",
        "\n",
        "                def clean_books(df):\n",
        "                    cleaned = df.copy()\n",
        "                    cleaned[\"Authors\"] = cleaned[\"Authors\"].fillna(\"Unknown\").str.title()\n",
        "                    cleaned[\"Categories\"] = cleaned[\"Categories\"].fillna(\"misc\").str.lower()\n",
        "                    cleaned[\"PublishedDate\"] = pd.to_datetime(cleaned[\"PublishedDate\"], errors=\"coerce\")\n",
        "                    cleaned[\"RatingsCount\"] = cleaned[\"RatingsCount\"].fillna(0).astype(int)\n",
        "                    return cleaned\n",
        "\n",
        "                def clean_reviews(df):\n",
        "                    cleaned = df.copy()\n",
        "                    cleaned.rename(columns={\"profileName\": \"ProfileName\"}, inplace=True)\n",
        "                    cleaned[\"review/text\"] = cleaned[\"review/text\"].fillna(\"\")\n",
        "                    cleaned[\"review/score\"] = cleaned[\"review/score\"].fillna(cleaned[\"review/score\"].mean())\n",
        "                    cleaned[\"review/time\"] = pd.to_datetime(cleaned[\"review/time\"], unit=\"s\", errors=\"coerce\")\n",
        "                    return cleaned\n",
        "\n",
        "                def enrich_features(df):\n",
        "                    enriched = df.copy()\n",
        "                    enriched[\"review_length\"] = enriched[\"review/text\"].str.split().map(len)\n",
        "                    enriched[\"CategoriesList\"] = (\n",
        "                        enriched[\"Categories\"].str.split(\"|\").apply(lambda values: [v.strip() for v in values if v])\n",
        "                    )\n",
        "                    return enriched\n",
        "\n",
        "                def compute_metrics(df):\n",
        "                    metrics: Dict[str, pd.DataFrame] = {}\n",
        "                    metrics[\"avg_rating_per_author\"] = (\n",
        "                        df.groupby(\"Authors\")[\"review/score\"].mean().reset_index().sort_values(\"review/score\", ascending=False)\n",
        "                    )\n",
        "                    metrics[\"reviews_per_publisher\"] = (\n",
        "                        df.groupby(\"Publisher\")[\"Id\"].count().reset_index().rename(columns={\"Id\": \"review_count\"})\n",
        "                    )\n",
        "                    exploded = df.explode(\"CategoriesList\")\n",
        "                    metrics[\"top_categories\"] = (\n",
        "                        exploded.groupby(\"CategoriesList\")[\"Id\"].count().reset_index().rename(\n",
        "                            columns={\"Id\": \"review_count\", \"CategoriesList\": \"Category\"}\n",
        "                        ).sort_values(\"review_count\", ascending=False).head(10)\n",
        "                    )\n",
        "                    metrics[\"review_length_stats\"] = pd.DataFrame(\n",
        "                        [\n",
        "                            {\"metric\": \"mean\", \"value\": df[\"review_length\"].mean()},\n",
        "                            {\"metric\": \"median\", \"value\": df[\"review_length\"].median()},\n",
        "                            {\"metric\": \"std\", \"value\": df[\"review_length\"].std()},\n",
        "                        ]\n",
        "                    )\n",
        "                    tokens = Counter(\" \".join(df[\"review/text\"]).split())\n",
        "                    metrics[\"top_keywords\"] = pd.DataFrame(tokens.most_common(15), columns=[\"keyword\", \"occurrences\"])\n",
        "                    return metrics\n",
        "\n",
        "                def persist_outputs(df, metrics: Dict[str, pd.DataFrame], path: Path, writer) -> None:\n",
        "                    try:\n",
        "                        writer(df, path)\n",
        "                    except Exception as write_error:\n",
        "                        print(f\"Failed to persist merged dataset: {write_error}\")\n",
        "                    else:\n",
        "                        for name, frame in metrics.items():\n",
        "                            target = ANALYSIS_DIR / f\"{path.stem}_{name}.csv\"\n",
        "                            try:\n",
        "                                frame.to_csv(target, index=False)\n",
        "                            except Exception as export_error:\n",
        "                                print(f\"Could not export metric {name}: {export_error}\")\n",
        "\n",
        "                def run_pipeline(format_name: str, writer_callable, project_name: str, output_name: str) -> Dict[str, object]:\n",
        "                    tracker = create_tracker(project_name)\n",
        "                    start = time.perf_counter()\n",
        "                    emissions = math.nan\n",
        "                    error: Optional[str] = None\n",
        "                    merged_df = None\n",
        "                    metrics: Dict[str, pd.DataFrame] = {}\n",
        "                    try:\n",
        "                        tracker.start()\n",
        "                        books_df = clean_books(pd.read_csv(books_path))\n",
        "                        reviews_df = clean_reviews(pd.read_csv(reviews_path))\n",
        "                        merged_df = enrich_features(reviews_df.merge(books_df, on=\"Title\", how=\"inner\"))\n",
        "                        metrics = compute_metrics(merged_df)\n",
        "                        persist_outputs(merged_df, metrics, OUTPUTS_DIR / output_name, writer_callable)\n",
        "                    except Exception as pipeline_error:\n",
        "                        error = str(pipeline_error)\n",
        "                        print(f\"[{format_name}] Pipeline encountered an issue: {pipeline_error}\")\n",
        "                    finally:\n",
        "                        duration = time.perf_counter() - start\n",
        "                        try:\n",
        "                            emissions = tracker.stop()\n",
        "                        except Exception as tracker_error:\n",
        "                            print(f\"[{format_name}] Unable to obtain emissions from tracker: {tracker_error}\")\n",
        "                        result = {\n",
        "                            \"format\": format_name,\n",
        "                            \"runtime_s\": duration,\n",
        "                            \"emissions_kg\": emissions,\n",
        "                            \"error\": error,\n",
        "                            \"row_count\": int(0 if merged_df is None else len(merged_df)),\n",
        "                        }\n",
        "                        result[\"metrics\"] = metrics\n",
        "                        PIPELINE_RESULTS.append(result)\n",
        "                        return result\n",
        "\n",
        "                def write_csv(df: pd.DataFrame, path: Path) -> None:\n",
        "                    df.to_csv(path, index=False)\n",
        "\n",
        "                def write_parquet(df: pd.DataFrame, path: Path) -> None:\n",
        "                    try:\n",
        "                        df.to_parquet(path, index=False, compression=\"snappy\")\n",
        "                    except Exception as parquet_error:\n",
        "                        print(f\"Snappy compression failed ({parquet_error}); retrying without compression.\")\n",
        "                        df.to_parquet(path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Task 1 \u2014 CSV Baseline ===\n",
        "if not DEPENDENCIES_READY:\n",
        "    print(\"CSV pipeline skipped because pandas/matplotlib are unavailable.\")\n",
        "else:\n",
        "    csv_result = run_pipeline(\"csv\", write_csv, \"csv_pipeline\", \"merged_books_reviews.csv\")\n",
        "    if csv_result.get(\"error\") is None:\n",
        "        pd.DataFrame([csv_result]).drop(columns=[\"metrics\"]).to_csv(\"emissions_csv.csv\", index=False)\n",
        "    csv_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline B \u2013 Parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Task 2 \u2014 Parquet Pipeline ===\n",
        "if not DEPENDENCIES_READY:\n",
        "    print(\"Parquet pipeline skipped because pandas/matplotlib are unavailable.\")\n",
        "else:\n",
        "    parquet_result = run_pipeline(\"parquet\", write_parquet, \"parquet_pipeline\", \"merged_books_reviews.parquet\")\n",
        "    if parquet_result.get(\"error\") is None:\n",
        "        pd.DataFrame([parquet_result]).drop(columns=[\"metrics\"]).to_csv(\"emissions_parquet.csv\", index=False)\n",
        "    parquet_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3 \u2014 Comparison and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not DEPENDENCIES_READY:\n",
        "    print(\"Comparison skipped because dependencies are missing.\")\n",
        "else:\n",
        "    summary_df = pd.DataFrame([\n",
        "        {k: v for k, v in result.items() if k not in (\"metrics\",)} for result in PIPELINE_RESULTS\n",
        "    ])\n",
        "    display(summary_df)\n",
        "    analysis_path = ANALYSIS_DIR / \"format_comparison.csv\"\n",
        "    summary_df.to_csv(analysis_path, index=False)\n",
        "\n",
        "    figure_path = ANALYSIS_DIR / \"format_comparison.png\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    summary_df.plot.bar(x=\"format\", y=\"runtime_s\", ax=axes[0], color=\"#1f77b4\")\n",
        "    axes[0].set_ylabel(\"Runtime (s)\")\n",
        "    axes[0].set_title(\"Runtime by format\")\n",
        "    summary_df.plot.bar(x=\"format\", y=\"emissions_kg\", ax=axes[1], color=\"#2ca02c\")\n",
        "    axes[1].set_ylabel(\"Emissions (kg CO\u2082)\")\n",
        "    axes[1].set_title(\"Emissions by format\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(figure_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"Comparison artefacts saved to {analysis_path} and {figure_path}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4 \u2014 Eco-Design Experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not DEPENDENCIES_READY:\n",
        "    print(\"Eco-design experiment skipped because dependencies are missing.\")\n",
        "else:\n",
        "    important_columns = [\"Id\", \"Title\", \"review/score\", \"review/text\", \"review_length\", \"Authors\", \"Categories\"]\n",
        "\n",
        "    def write_filtered_parquet(df: pd.DataFrame, path: Path) -> None:\n",
        "        filtered = df[important_columns]\n",
        "        try:\n",
        "            filtered.to_parquet(path, index=False, compression=\"snappy\")\n",
        "        except Exception as parquet_error:\n",
        "            print(f\"Filtered export fallback (no snappy): {parquet_error}\")\n",
        "            filtered.to_parquet(path, index=False)\n",
        "\n",
        "    filtered_result = run_pipeline(\"parquet_filtered\", write_filtered_parquet, \"parquet_filtered\", \"merged_filtered.parquet\")\n",
        "    filtered_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Before vs After optimization\n",
        "\n",
        "- Removed non-essential columns before saving the optimised Parquet artefact.\n",
        "- The resulting file is smaller and quicker to write/read for downstream tasks.\n",
        "- Shorter write duration yields a lower estimated energy footprint.\n",
        "- Compression still applies, so CPU work rises slightly but net emissions decrease.\n",
        "- Highlights that thoughtful schema design complements format selection in eco-design.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection (10 lines)\n",
        "\n",
        "1. Switching from CSV to Parquet demonstrates tangible runtime improvements on analytical joins.\n",
        "2. Column pruning delivers an additional benefit even when Parquet is already compact.\n",
        "3. Measuring energy with CodeCarbon (or a fallback) keeps sustainability visible during development.\n",
        "4. Synthetic data makes the notebook reproducible without external downloads.\n",
        "5. Cleaning steps standardise authors and categories, enabling consistent aggregations.\n",
        "6. Keyword extraction from reviews surfaces qualitative signals beyond numeric ratings.\n",
        "7. Persisting comparison artefacts in `analysis/` simplifies reporting across reruns.\n",
        "8. Try/except/finally blocks guarantee trackers stop even when something fails mid-pipeline.\n",
        "9. Visual comparisons translate tabular metrics into quicker insights for stakeholders.\n",
        "10. The exercise highlights how eco-design complements, rather than replaces, classical optimisation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "- Parquet artefacts are dramatically smaller than their CSV counterparts.\n",
        "- End-to-end runtime improves thanks to reduced I/O and efficient encoding.\n",
        "- Choosing the right storage format is a practical lever for greener data engineering.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}